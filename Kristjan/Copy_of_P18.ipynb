{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of P18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szuAyA1TJHxV"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pez7kG4AXXwp"
      },
      "source": [
        "Let us first separate only the neccesary colums, combine the data into one collection, and covert it to binary labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6HafrWEjPuG",
        "outputId": "4e451f9f-1cd9-4882-e2af-df54a5eb6280"
      },
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Machine Learning/project/Karl'\n",
        "os.chdir(path)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3MWKuToJMES"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "am = pd.read_csv('../adverse_media_training.csv')\n",
        "nam = pd.read_csv('../non_adverse_media_training.csv')\n",
        "\n",
        "# Keep only needed columns\n",
        "am_cropped = am[['article','title','label']]\n",
        "nam_cropped = nam[['article','title', 'label']]\n",
        "\n",
        "# Combine source files and re-label to binary\n",
        "am = pd.concat(\n",
        "    [ am_cropped.loc[(am_cropped.label == 'am') | (am_cropped.label == 'am ')],\n",
        "     nam_cropped.loc[(nam_cropped.label == 'am')] ]\n",
        ")\n",
        "am['label'] = 1\n",
        "\n",
        "nam = pd.concat(\n",
        "    [ am_cropped.loc[(am_cropped.label == 'nam') | (am_cropped.label == 'random')], \n",
        "     nam_cropped.loc[(nam_cropped.label == 'nam')] ]\n",
        ")\n",
        "nam['label'] = 0\n",
        "\n",
        "\n",
        "# Combine data into one table\n",
        "data = pd.concat([am,nam])\n",
        "\n",
        "data[\"article\"] = data[\"title\"] + \" \" + data[\"article\"]\n",
        "data = data.drop([\"title\"], axis =1)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTAucEGOXcR3"
      },
      "source": [
        "Now let us combine the article and title columns, remove punctuation, lowercase the text, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnRa1q4sXg2S"
      },
      "source": [
        "import spacy\r\n",
        "import re\r\n",
        "\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "\r\n",
        "# Should be (almost) the same as Canberk's, but slighlty faster, as not compiling the regex each time\r\n",
        "regex1 = re.compile(r'(http\\S+)|(#(\\w+))|(@(\\w+))|[^\\w\\s]|(\\w*\\d\\w*)')\r\n",
        "regex2 = re.compile(r'(\\s+)|(\\n+)')\r\n",
        "\r\n",
        "def lemmatize(article):\r\n",
        "    article = re.sub(regex1, '', article)\r\n",
        "    article = re.sub(regex2,' ', article).strip().lower()\r\n",
        "    \r\n",
        "    doc = nlp(article)\r\n",
        "    lemmatized_article = \" \".join([token.lemma_ for token in doc if (token.is_stop==False)]) \r\n",
        "    \r\n",
        "    return lemmatized_article"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5in-fuQXwQh"
      },
      "source": [
        "Lemmatizing the whole dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "sPLSVCG7Xxtx",
        "outputId": "35ce9eb3-b420-49be-dee5-c3e5042464cc"
      },
      "source": [
        "train = data[['article', 'label']].copy()\r\n",
        "train[\"article\"] = train[\"article\"].apply(lemmatize)\r\n",
        "train = train.reset_index()\r\n",
        "train = train.drop(['index'], axis=1)\r\n",
        "train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>crooked ceos bernie madoff schedule sentence j...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fund manager force resign bbc investigation pu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>peregrine financial group boss admit fraud pub...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>american accuse congo official unlawful arrest...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bitcoin foundation vice chair arrest money lau...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>721</th>\n",
              "      <td>lead uk bank strengthen fight rise payment fra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>722</th>\n",
              "      <td>shadow chancellor call minister fulfil pledge ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>723</th>\n",
              "      <td>peru oust president threaten rule law washingt...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>france give online firm hour pull terrorist co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>congressman urge vigilance anniversary antitra...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>726 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               article  label\n",
              "0    crooked ceos bernie madoff schedule sentence j...      1\n",
              "1    fund manager force resign bbc investigation pu...      1\n",
              "2    peregrine financial group boss admit fraud pub...      1\n",
              "3    american accuse congo official unlawful arrest...      1\n",
              "4    bitcoin foundation vice chair arrest money lau...      1\n",
              "..                                                 ...    ...\n",
              "721  lead uk bank strengthen fight rise payment fra...      0\n",
              "722  shadow chancellor call minister fulfil pledge ...      0\n",
              "723  peru oust president threaten rule law washingt...      0\n",
              "724  france give online firm hour pull terrorist co...      0\n",
              "725  congressman urge vigilance anniversary antitra...      0\n",
              "\n",
              "[726 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzvGifRY_Lec"
      },
      "source": [
        " https://www.analyticsvidhya.com/blog/2020/10/simple-text-multi-classification-task-using-keras-bert/\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAwubgSU_N40"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtp5kzLy_tE5",
        "outputId": "19f4203d-db7a-410a-8a05-8879119887d3"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 18.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 15.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 17.6MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 14.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 15.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 15.1MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 13.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 13.8MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 13.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 13.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 440kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 471kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 552kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 583kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 614kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 634kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 645kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 675kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 706kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 727kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 757kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 778kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 788kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 808kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 819kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 849kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 880kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890kB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 901kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 911kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921kB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 931kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 942kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 952kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 962kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 983kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 993kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0MB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.0MB 13.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0MB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1MB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1MB 13.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.1MB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1MB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 13.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 13.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq4UN2LU_Xpf"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import logging\r\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO9ERqYj_P42",
        "outputId": "9ef4da1d-a48b-4202-bdfb-7937b4b8bc3b"
      },
      "source": [
        "import tensorflow_hub as hub\r\n",
        "import tokenization\r\n",
        "# module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\r\n",
        "module_path = 'bert-layer'\r\n",
        "bert_layer = hub.KerasLayer(module_path, trainable=True)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:resolver HttpCompressedFileResolver does not support the provided handle.\n",
            "INFO:absl:resolver GcsCompressedFileResolver does not support the provided handle.\n",
            "INFO:absl:resolver HttpUncompressedFileResolver does not support the provided handle.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q84zD1hVAZ3"
      },
      "source": [
        "# Possible improvement: \r\n",
        "Read TODO comment below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6PsqSr6ACt0"
      },
      "source": [
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\r\n",
        "\r\n",
        "def bert_encode(texts, tokenizer, max_len=512):\r\n",
        "    all_tokens = []\r\n",
        "    all_masks = []\r\n",
        "    all_segments = []\r\n",
        "    \r\n",
        "    for text in texts:\r\n",
        "        text = tokenizer.tokenize(text)\r\n",
        "            \r\n",
        "\r\n",
        "        # TODO: Should be changed to split the text into chunks, process each chunk separately, and later combine\r\n",
        "        text = text[:max_len-2]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\r\n",
        "        pad_len = max_len - len(input_sequence)\r\n",
        "        \r\n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\r\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\r\n",
        "        segment_ids = [0] * max_len\r\n",
        "        \r\n",
        "        all_tokens.append(tokens)\r\n",
        "        all_masks.append(pad_masks)\r\n",
        "        all_segments.append(segment_ids)\r\n",
        "    \r\n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkF95aFNVWbD"
      },
      "source": [
        "# Possible improvents: \r\n",
        "Add layers, change params, replace with any model really, but it works for now just keep the \"bert_layer\" in there as one of the first steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBcsMMrsBq69"
      },
      "source": [
        "def build_model(bert_layer, max_len=512):\r\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\r\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\r\n",
        "    \r\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\r\n",
        "    clf_output = sequence_output[:, 0, :]\r\n",
        "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\r\n",
        "    net = tf.keras.layers.Dropout(0.2)(net)\r\n",
        "    net = tf.keras.layers.Dense(32, activation='relu')(net)\r\n",
        "    net = tf.keras.layers.Dropout(0.2)(net)\r\n",
        "    out = tf.keras.layers.Dense(2, activation='softmax')(net)\r\n",
        "    \r\n",
        "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\r\n",
        "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XaBn8UZCEHC",
        "outputId": "0c159e4a-da71-406c-e436-851befaa7025"
      },
      "source": [
        "## test-train split: \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "train = pd.read_csv('all_lemmatized.csv', lineterminator='\\n').iloc[:, 1:3]\r\n",
        "\r\n",
        "bert_train = train.sample(frac = 1) \r\n",
        "\r\n",
        "x_train, x_val, y_train, y_val = train_test_split(bert_train['article'], \r\n",
        "                                                    bert_train['label'], \r\n",
        "                                                    test_size=0.1, \r\n",
        "                                                    random_state=42,\r\n",
        "                                                    stratify= bert_train['label'])\r\n",
        "\r\n",
        "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(642,) (72,) (642,) (72,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTZQjhNQVxer"
      },
      "source": [
        "# Increase the max_len param for better results, but more time taken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLWCDI2KBvSs"
      },
      "source": [
        "import keras\r\n",
        "max_len = 500 # Larger takes longer\r\n",
        "train_input = bert_encode(x_train, tokenizer, max_len=max_len)\r\n",
        "test_input = bert_encode(x_val, tokenizer, max_len=max_len)\r\n",
        "train_labels = keras.utils.to_categorical(y_train, num_classes=2)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfrQ6CRVEMsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89688bb3-ee83-4ddb-8d6b-a3f4ce14539f"
      },
      "source": [
        "model = build_model(bert_layer, max_len=max_len)\r\n",
        "model.summary()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_3 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_2 (Te [(None, 768)]        0           keras_layer_3[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 64)           49216       tf_op_layer_strided_slice_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 64)           0           dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 32)           2080        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32)           0           dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 2)            66          dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,533,603\n",
            "Trainable params: 109,533,602\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXf6zYuyEQSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b75d75-5757-4339-a43b-47ac1426fbbc"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model-again.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\r\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)\r\n",
        "\r\n",
        "train_history = model.fit(\r\n",
        "    train_input, train_labels, \r\n",
        "    validation_split=0.2,\r\n",
        "    epochs=10,\r\n",
        "    callbacks=[checkpoint, earlystopping],\r\n",
        "    batch_size=8,\r\n",
        "    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.1440 - accuracy: 0.9571\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89922, saving model to model.h5\n",
            "65/65 [==============================] - 50s 776ms/step - loss: 0.1440 - accuracy: 0.9571 - val_loss: 0.4851 - val_accuracy: 0.8992\n",
            "Epoch 2/10\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9844\n",
            "Epoch 00002: val_accuracy did not improve from 0.89922\n",
            "65/65 [==============================] - 38s 588ms/step - loss: 0.0724 - accuracy: 0.9844 - val_loss: 0.5866 - val_accuracy: 0.8682\n",
            "Epoch 3/10\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9942\n",
            "Epoch 00003: val_accuracy did not improve from 0.89922\n",
            "65/65 [==============================] - 38s 588ms/step - loss: 0.0258 - accuracy: 0.9942 - val_loss: 0.5888 - val_accuracy: 0.8992\n",
            "Epoch 4/10\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9981\n",
            "Epoch 00004: val_accuracy did not improve from 0.89922\n",
            "65/65 [==============================] - 38s 588ms/step - loss: 0.0153 - accuracy: 0.9981 - val_loss: 0.6655 - val_accuracy: 0.8992\n",
            "Epoch 5/10\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9961\n",
            "Epoch 00005: val_accuracy did not improve from 0.89922\n",
            "65/65 [==============================] - 38s 587ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.8212 - val_accuracy: 0.8915\n",
            "Epoch 6/10\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9961\n",
            "Epoch 00006: val_accuracy did not improve from 0.89922\n",
            "65/65 [==============================] - 38s 587ms/step - loss: 0.0142 - accuracy: 0.9961 - val_loss: 0.8348 - val_accuracy: 0.8760\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7Wug9MJ19i"
      },
      "source": [
        "model.load_weights('model.h5')\r\n",
        "test_pred = model.predict(test_input)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj-ER0suUTIX",
        "outputId": "3d81e4d3-82b2-488d-934e-42ed9ae179c5"
      },
      "source": [
        "# from probability to binary\r\n",
        "pred = [1 if el[1]> 0.5 else 0 for el in test_pred]\r\n",
        "pred[:5]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWW-a5MERe3n",
        "outputId": "f495e2be-59b5-4a94-b9e4-f6a0f6d17215"
      },
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "\r\n",
        "\r\n",
        "val_f1_score = f1_score(y_val, pred)\r\n",
        "\r\n",
        "print('F1 score for model on validation data:', round(val_f1_score*100, 3))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score for model on validation data: 93.506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "H6hNvjJrmbaY",
        "outputId": "5d2014a8-3402-4f0a-d6b4-6c4ed0fcbfd0"
      },
      "source": [
        "public_test = pd.read_csv('../public_test.csv')\n",
        "public_test"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>article</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>931</td>\n",
              "      <td>Caputo concealed Cayman Island offshore firms ...</td>\n",
              "      <td>By Sandra Crucianelli, Emilia Delfino y From B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>644</td>\n",
              "      <td>California Man Pleads Guilty in $6 Million Art...</td>\n",
              "      <td>A California man pleaded guilty in federal cou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>881</td>\n",
              "      <td>Couple jailed for laundering £50m</td>\n",
              "      <td>A couple who ran a diamond trading business ha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>841</td>\n",
              "      <td>John Gilligan charged with money laundering of...</td>\n",
              "      <td>image copyrightRTÉ\\n\\nA Dublin man has been ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>Grace Mugabe faces arrest in Mary Chiwenga Sty...</td>\n",
              "      <td>Zimbabwe News\\n\\nGrace Mugabe faces arrest in ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>348</td>\n",
              "      <td>Kanye West's strange presidential bid unravels...</td>\n",
              "      <td>(CNN) Kanye West is on the ballot in Minnesota...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>295</td>\n",
              "      <td>Anti-money laundering software startup TookiTa...</td>\n",
              "      <td>TookiTaki, a startup that develops machine lea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>311</td>\n",
              "      <td>If we really want to know what makes terrorist...</td>\n",
              "      <td>In the last two and half years I’ve studied th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>545</td>\n",
              "      <td>An effective e-declaration system will be a wa...</td>\n",
              "      <td>BY MARCUS BRAND,\\n\\nTwo-and-a-half years ago, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>560</td>\n",
              "      <td>Building legal frameworks to protect people wh...</td>\n",
              "      <td>Across Europe, Transparency International (TI)...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... label\n",
              "0    931  ...     1\n",
              "1    644  ...     1\n",
              "2    881  ...     1\n",
              "3    841  ...     1\n",
              "4     31  ...     1\n",
              "..   ...  ...   ...\n",
              "154  348  ...     0\n",
              "155  295  ...     0\n",
              "156  311  ...     0\n",
              "157  545  ...     0\n",
              "158  560  ...     0\n",
              "\n",
              "[159 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "UHHpZmUGmSS5",
        "outputId": "9e38553a-d1ec-4c9e-ea6b-a8f5e445e851"
      },
      "source": [
        "public_test[\"article\"] = public_test[\"title\"] + \" \" + public_test[\"article\"]\n",
        "public_test.drop([\"title\"], axis =1)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>article</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>931</td>\n",
              "      <td>Caputo concealed Cayman Island offshore firms ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>644</td>\n",
              "      <td>California Man Pleads Guilty in $6 Million Art...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>881</td>\n",
              "      <td>Couple jailed for laundering £50m A couple who...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>841</td>\n",
              "      <td>John Gilligan charged with money laundering of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>Grace Mugabe faces arrest in Mary Chiwenga Sty...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>348</td>\n",
              "      <td>Kanye West's strange presidential bid unravels...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>295</td>\n",
              "      <td>Anti-money laundering software startup TookiTa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>311</td>\n",
              "      <td>If we really want to know what makes terrorist...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>545</td>\n",
              "      <td>An effective e-declaration system will be a wa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>560</td>\n",
              "      <td>Building legal frameworks to protect people wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                            article  label\n",
              "0    931  Caputo concealed Cayman Island offshore firms ...      1\n",
              "1    644  California Man Pleads Guilty in $6 Million Art...      1\n",
              "2    881  Couple jailed for laundering £50m A couple who...      1\n",
              "3    841  John Gilligan charged with money laundering of...      1\n",
              "4     31  Grace Mugabe faces arrest in Mary Chiwenga Sty...      1\n",
              "..   ...                                                ...    ...\n",
              "154  348  Kanye West's strange presidential bid unravels...      0\n",
              "155  295  Anti-money laundering software startup TookiTa...      0\n",
              "156  311  If we really want to know what makes terrorist...      0\n",
              "157  545  An effective e-declaration system will be a wa...      0\n",
              "158  560  Building legal frameworks to protect people wh...      0\n",
              "\n",
              "[159 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "v0-n9LX3mZTM",
        "outputId": "66e88429-a1bc-4625-e867-531ecc034640"
      },
      "source": [
        "public_test_lemmatized = public_test[['article', 'label']].copy()\n",
        "public_test_lemmatized[\"article\"] = public_test_lemmatized[\"article\"].apply(lemmatize)\n",
        "public_test_lemmatized = public_test_lemmatized.reset_index()\n",
        "public_test_lemmatized = public_test_lemmatized.drop(['index'], axis=1)\n",
        "public_test_lemmatized"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>caputo conceal cayman island offshore firm arg...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>california man plead guilty million art fraud ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>couple jail launder couple run diamond trading...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>john gilligan charge money laundering offence ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>grace mugabe face arrest mary chiwenga style s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>kanye west strange presidential bid unravel th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>antimoney laundering software startup tookitak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>want know make terrorist commit atrocity half ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>effective edeclaration system watershed countr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>build legal framework protect people reveal co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               article  label\n",
              "0    caputo conceal cayman island offshore firm arg...      1\n",
              "1    california man plead guilty million art fraud ...      1\n",
              "2    couple jail launder couple run diamond trading...      1\n",
              "3    john gilligan charge money laundering offence ...      1\n",
              "4    grace mugabe face arrest mary chiwenga style s...      1\n",
              "..                                                 ...    ...\n",
              "154  kanye west strange presidential bid unravel th...      0\n",
              "155  antimoney laundering software startup tookitak...      0\n",
              "156  want know make terrorist commit atrocity half ...      0\n",
              "157  effective edeclaration system watershed countr...      0\n",
              "158  build legal framework protect people reveal co...      0\n",
              "\n",
              "[159 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjsdtwLpm1_6",
        "outputId": "c7028fb2-e085-4cf8-8936-b529e00b7f19"
      },
      "source": [
        "\n",
        "public_test_tokenized = bert_encode(public_test_lemmatized['article'], tokenizer, max_len=max_len)\n",
        "public_test_pred = model.predict(public_test_tokenized)\n",
        "\n",
        "public_test_pred = [1 if el[1]> 0.5 else 0 for el in public_test_pred]\n",
        "\n",
        "public_test_f1_score = f1_score(public_test_lemmatized['label'], public_test_pred)\n",
        "\n",
        "print('F1 score for model on public test data:', round(public_test_f1_score*100, 3))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score for model on public test data: 94.301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SXEMOVMnydN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}